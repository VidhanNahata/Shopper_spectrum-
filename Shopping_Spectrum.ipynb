{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "Y3lxredqlCYt",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VidhanNahata/Shopper_spectrum-/blob/main/Shopping_Spectrum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Shopper spectrum\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    -Unsupervised\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rapid expansion of the global e-commerce industry has resulted in the generation of immense volumes of transactional data every day. This data not only reflects a company's sales activities but also holds significant potential for uncovering deep insights into consumer purchasing behavior. By systematically analyzing transaction data, businesses can identify customer preferences, buying habits, and emerging market trends, informing strategic decisions and providing tailored experiences that fuel both customer satisfaction and sustained business growth.\n",
        "\n",
        "A central challenge within this context is to segment customers effectively and recommend products they are more likely to purchase. Segmentation is crucial because not all customers have the same value to a business. Understanding and classifying customers based on their interactions and monetary contributions allows companies to allocate resources more efficiently and design targeted marketing campaigns. One robust and widely used technique for this purpose is RFM (Recency, Frequency, and Monetary) analysis.\n",
        "\n",
        "RFM analysis breaks down customer behavior into three key dimensions:\n",
        "\n",
        "Recency refers to how recently a customer has made a purchase, signaling ongoing engagement and propensity to buy again.\n",
        "\n",
        "Frequency measures how often a customer purchases within a given period, highlighting repeat buyers versus one-time shoppers.\n",
        "\n",
        "Monetary evaluates the total amount a customer has spent, identifying high-value individuals who substantially impact revenue.\n",
        "\n",
        "By scoring customers along these three axes and clustering them into meaningful segments (such as loyal customers, potential loyalists, and at-risk customers), businesses can prioritize retention efforts, upsell strategies, and personalized promotions with higher precision.\n",
        "\n",
        "Beyond segmentation, personalized product recommendations have become integral to modern e-commerce platforms. Recommender systems help customers discover products they might not otherwise find, improving the shopping experience and encouraging additional purchases. Among the various techniques used, collaborative filtering stands out for its effectiveness. Collaborative filtering leverages the wisdom of the crowd by analyzing patterns in user behavior and identifying similarities between users or items.\n",
        "\n",
        "This approach generally falls into two categories:\n",
        "\n",
        "User-based collaborative filtering: Recommends products to a user based on what similar users have liked or purchased.\n",
        "\n",
        "Item-based collaborative filtering: Suggests items similar to those a user has interacted with, based on aggregate patterns across the user base.\n",
        "\n",
        "By applying collaborative filtering to e-commerce transaction data, platforms can deliver highly relevant product suggestions, even in the absence of explicit user preferences. These recommendations can increase conversion rates, average order values, and foster long-term customer loyalty.\n",
        "\n",
        "Integrating RFM-based segmentation with collaborative filtering further refines personalization. For example, different customer segments may receive tailored product recommendations or marketing messages: high-value repeat buyers could receive early access to new arrivals, while recent but infrequent purchasers might get bundled offers to encourage repeat business. Such granularity increases the effectiveness of marketing interventions and ensures that resources are focused where they are likely to deliver maximum impact.\n",
        "\n",
        "The effectiveness of these methods hinges on rigorous data preprocessing, including handling missing values, removing anomalies, and structuring the data for meaningful interpretation. Visualization techniques such as heatmaps, cluster plots, and time-series analyses can make sense of complex behavioral patterns and demonstrate the distinctiveness of discovered segments.\n",
        "\n",
        "In summary, the project explores a holistic approach to leveraging e-commerce transaction data, combining RFM segmentation with collaborative filtering to unlock the full potential of customer analytics. This dual strategy not only improves the relevance of product recommendations but also empowers businesses to nurture profitable customer relationships, boosting both customer satisfaction and revenue growth. By continuously refining these models using fresh data and feedback, e-commerce businesses can maintain a competitive edge in today’s data-driven marketplace."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/VidhanNahata/Shopper_spectrum-"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The global e-commerce industry generates vast amounts of transaction data daily, offering valuable insights into customer purchasing behaviors. Analyzing this data is essential for identifying meaningful customer segments and recommending relevant products to enhance customer experience and drive business growth. This project aims to examine transaction data from an online retail business to uncover patterns in customer purchase behavior, segment customers based on Recency, Frequency, and Monetary (RFM) analysis, and develop a product recommendation system using collaborative filtering techniques."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from datetime import timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "HfE1ONfKBWJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('online_retail.csv')\n"
      ],
      "metadata": {
        "id": "7N-JasTTCAZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "DYzmk-9QDy3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "dAPMvm9YH4RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(f\"Number of duplicate rows: {df.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(\"Missing values per column:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "cLC8nDnOIsFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Country'].unique()"
      ],
      "metadata": {
        "id": "lYEehyMWIvBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Country'].value_counts()"
      ],
      "metadata": {
        "id": "NJ7xYePNIyRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It contains 541,909 rows and 8 columns.\n",
        "\n",
        "The columns are: InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, and Country.\n",
        "\n",
        "There are 5,268 duplicate rows.\n",
        "\n",
        "There are missing values in the Description (1454 missing values) and CustomerID (135080 missing values) columns.\n",
        "\n",
        "The data types are a mix of object, int64, and float64. The InvoiceDate column is currently an object type and will need to be converted to a datetime object for time-based analysis.\n",
        "\n",
        "The Quantity column contains negative values, indicating potential returns or cancellations.\n",
        "\n",
        "The UnitPrice column contains zero and negative values, which may need investigation.\n",
        "\n",
        "The Country column contains data for 38 different countries, with the 'United Kingdom' being the most frequent.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DROPPING ROWS WITH MISSING CUSTOMERID"
      ],
      "metadata": {
        "id": "x3nyTIb1KWL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(subset=['CustomerID'], inplace=True)"
      ],
      "metadata": {
        "id": "NXBClNIZKYtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "ogCth3JWKmbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "NuKo3EXKKp3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DROPPING DUPLICATED DATA"
      ],
      "metadata": {
        "id": "EHE0LQDsKv0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()\n",
        "\n",
        "df = df.drop_duplicates()\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "Jx9W6lYtKvhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXCLUDE CANCELLED INVOICES(INVOICE_NO STARTING WITH 'C')"
      ],
      "metadata": {
        "id": "5Gl5l6Q0LBvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]\n",
        "df['InvoiceNo'].value_counts()"
      ],
      "metadata": {
        "id": "zfP9WLgDLBKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "REMOVE NEGATIVE OR ZERO QUANTITIES AND PRICES"
      ],
      "metadata": {
        "id": "jYCCqV1rLKrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[(df['Quantity']>0) & (df['UnitPrice']>0)]\n",
        "df"
      ],
      "metadata": {
        "id": "61E_nsghLLfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXPLORATORY DATA ANALYSIS(EDA)"
      ],
      "metadata": {
        "id": "jCzbC_K_LUCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "rcySBitOLYdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze transaction volume by country\n"
      ],
      "metadata": {
        "id": "UdxcuNLTL24I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trans_by_country = df.groupby('Country')['InvoiceNo'].nunique().sort_values(ascending=False)\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x=trans_by_country.index, y=trans_by_country.values, palette='viridis')\n",
        "plt.title('Number of Transactions by Country')\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Number of Transactions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VIQUbFPaMA3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Identify top-selling products"
      ],
      "metadata": {
        "id": "JzvIbOXVL7J3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_products = df.groupby('Description')['Quantity'].sum().sort_values(ascending=False).head(20)\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(y=top_products.index, x=top_products.values, palette='magma')\n",
        "plt.title('Top 20 Selling Products by Quantity')\n",
        "plt.xlabel('Total Quantity Sold')\n",
        "plt.ylabel('Product Description')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7_wH4eGbMibO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize purchase trends over time"
      ],
      "metadata": {
        "id": "twL9rGbpMB1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "df['Month'] = df['InvoiceDate'].dt.to_period('M')\n",
        "monthly_sales = df.groupby('Month')['Quantity'].sum()\n",
        "monthly_transactions = df.groupby('Month')['InvoiceNo'].nunique()\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "sns.lineplot(x=monthly_sales.index.astype(str), y=monthly_sales.values, label='Total Quantity Sold')\n",
        "sns.lineplot(x=monthly_transactions.index.astype(str), y=monthly_transactions.values, label='Total Transactions')\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('Monthly Purchase Trends')\n",
        "plt.ylabel('Count')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "982GNriNMuo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspect monetary distribution per transaction and customer\n"
      ],
      "metadata": {
        "id": "xRopmZEuME-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate TotalPrice for each row\n",
        "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "# Calculate Revenue per Transaction\n",
        "transaction_revenue = df.groupby('InvoiceNo')['TotalPrice'].sum()\n",
        "\n",
        "# Calculate Total Revenue per Customer\n",
        "customer_revenue = df.groupby('CustomerID')['TotalPrice'].sum()\n",
        "\n",
        "monetary_data = [\n",
        "    transaction_revenue.values,        # Revenue per Transaction\n",
        "    customer_revenue.dropna().values   # Revenue per Customer\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.violinplot(data=monetary_data, palette=['#1f77b4', '#ff7f0e'])\n",
        "plt.xticks([0, 1], ['Revenue per Transaction', 'Total Revenue per Customer'])\n",
        "plt.title('Monetary Distribution: Transactions vs Customers')\n",
        "plt.ylabel('Revenue')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3TjC6tQUMvI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RFM distributions\n"
      ],
      "metadata": {
        "id": "xNlztVthMHT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "snapshot_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
        "\n",
        "rfm = df.groupby('CustomerID').agg({\n",
        "    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,\n",
        "    'InvoiceNo': 'nunique',\n",
        "    'TotalPrice': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "rfm.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']\n",
        "\n",
        "# Plot RFM distributions\n",
        "plt.figure(figsize=(15,4))\n",
        "plt.subplot(1,3,1)\n",
        "sns.histplot(rfm['Recency'], bins=50, color='blue', kde=True)\n",
        "plt.title('Recency Distribution')\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "sns.histplot(rfm['Frequency'], bins=50, color='green', kde=True)\n",
        "plt.title('Frequency Distribution')\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "sns.histplot(rfm['Monetary'], bins=50, color='red', kde=True)\n",
        "plt.title('Monetary Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yK79JlRiMvgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Elbow curve for cluster selection"
      ],
      "metadata": {
        "id": "jkpnQBBJMJ_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "rfm_scaled = rfm[['Recency', 'Frequency', 'Monetary']].copy()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "rfm_scaled = scaler.fit_transform(rfm_scaled)\n",
        "\n",
        "sse = []\n",
        "k_range = range(1, 11)\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(rfm_scaled)\n",
        "    sse.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(k_range, sse, 'bx-')\n",
        "plt.xlabel('Number of clusters K')\n",
        "plt.ylabel('Sum of squared errors (SSE)')\n",
        "plt.title('Elbow Curve for Optimal K')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jJ6zeZGgMv6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Customer cluster profiles\n"
      ],
      "metadata": {
        "id": "7fsMYgCkMME5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_optimal = 4\n",
        "kmeans = KMeans(n_clusters=k_optimal, random_state=42, n_init=10) # Added n_init for clarity and to avoid warning\n",
        "rfm['Cluster'] = kmeans.fit_predict(rfm_scaled)\n",
        "\n",
        "# Summary stats per cluster\n",
        "cluster_summary = rfm.groupby('Cluster').agg({\n",
        "    'Recency': ['mean', 'median'],\n",
        "    'Frequency': ['mean', 'median'],\n",
        "    'Monetary': ['mean', 'median', 'count']\n",
        "}).round(1)\n",
        "print(cluster_summary)\n",
        "\n",
        "# Visualize clusters\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.scatterplot(data=rfm, x='Recency', y='Monetary', hue='Cluster', palette='Set1', alpha=0.6)\n",
        "plt.title('Customer Segments based on Recency and Monetary')\n",
        "plt.xlabel('Recency (days)')\n",
        "plt.ylabel('Monetary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S0IzM8okPFq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Product recommendation heatmap / similarity matrix"
      ],
      "metadata": {
        "id": "amrtt2zzMOu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_product = df.pivot_table(index='CustomerID', columns='Description', values='Quantity', aggfunc='sum', fill_value=0)\n",
        "\n",
        "# Compute cosine similarity between products\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "product_sim = cosine_similarity(customer_product.T)\n",
        "product_sim_df = pd.DataFrame(product_sim, index=customer_product.columns, columns=customer_product.columns)\n",
        "\n",
        "# For visualization, pick top 20 products by total sales\n",
        "top_20_products = top_products.index.tolist()\n",
        "sim_subset = product_sim_df.loc[top_20_products, top_20_products]\n",
        "\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(sim_subset, cmap='coolwarm', annot=False)\n",
        "plt.title('Product Similarity Heatmap (Top 20 Products)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z7-QVKQkMwc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering Methodology:\n"
      ],
      "metadata": {
        "id": "DDxMPAC-SEOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering:"
      ],
      "metadata": {
        "id": "826C2aJSSIie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "\n",
        "# Load data and preprocess\n",
        "df = pd.read_csv('online_retail.csv', encoding='latin1')\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "\n",
        "# Remove cancelled transactions (InvoiceNo starting with 'C')\n",
        "df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]\n",
        "\n",
        "# Set snapshot date for Recency calculation (one day after last purchase date)\n",
        "snapshot_date = df['InvoiceDate'].max() + timedelta(days=1)\n",
        "\n",
        "# Calculate RFM metrics\n",
        "rfm = df.groupby('CustomerID').agg({\n",
        "    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,  # Recency\n",
        "    'InvoiceNo': 'nunique'                                   # Frequency\n",
        "})\n",
        "\n",
        "rfm['Monetary'] = df.groupby('CustomerID').apply(lambda x: (x['Quantity'] * x['UnitPrice']).sum())\n",
        "\n",
        "# Rename columns for clarity\n",
        "rfm.rename(columns={'InvoiceDate': 'Recency', 'InvoiceNo': 'Frequency'}, inplace=True)\n",
        "\n",
        "print(rfm.head())"
      ],
      "metadata": {
        "id": "TgrfQSFDSbxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standardize/Normalize the RFM values"
      ],
      "metadata": {
        "id": "hL0_O885SO6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize RFM columns (Recency, Frequency, Monetary)\n",
        "scaler = StandardScaler()\n",
        "rfm_scaled = scaler.fit_transform(rfm)\n",
        "\n",
        "# Optionally, inspect the scaled data\n",
        "import numpy as np\n",
        "print(np.round(rfm_scaled[:5], 2))"
      ],
      "metadata": {
        "id": "eMje-dp2S6UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choose Clustering Algorithm (KMeans, DBScan, Hierarchial etc)"
      ],
      "metadata": {
        "id": "5LQdgHhjTNTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KMeans Clustering"
      ],
      "metadata": {
        "id": "VlQBM5fYTRa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "k = 5  # Update this with your chosen value based on elbow/silhouette analysis\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "kmeans.fit(rfm_scaled)\n",
        "rfm['KMeans_Cluster'] = kmeans.labels_\n",
        "\n",
        "print(\"KMeans Cluster assignments (first 5):\")\n",
        "print(rfm['KMeans_Cluster'].head())\n",
        "\n",
        "# Optionally: Plot cluster centers (in scaled data)\n",
        "print(\"KMeans cluster centers (scaled RFM):\")\n",
        "print(kmeans.cluster_centers_)"
      ],
      "metadata": {
        "id": "B_Mux50STpnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBScan clustering"
      ],
      "metadata": {
        "id": "OMhkfPJeTbES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# You may need to tune eps and min_samples depending on your data distribution\n",
        "dbscan = DBSCAN(eps=1, min_samples=5)\n",
        "rfm['DBSCAN_Cluster'] = dbscan.fit_predict(rfm_scaled)\n",
        "\n",
        "print(\"DBSCAN Cluster assignments (first 5):\")\n",
        "print(rfm['DBSCAN_Cluster'].head())\n",
        "print(\"DBSCAN labels unique:\", set(rfm['DBSCAN_Cluster']))"
      ],
      "metadata": {
        "id": "prNMlwngTp_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hierarchial clusteing"
      ],
      "metadata": {
        "id": "6xxZB6lbTau0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Try different n_clusters; start with 4 for consistency with KMeans\n",
        "agg = AgglomerativeClustering(n_clusters=4)\n",
        "rfm['Hierarchical_Cluster'] = agg.fit_predict(rfm_scaled)\n",
        "\n",
        "print(\"Hierarchical Cluster assignments (first 5):\")\n",
        "print(rfm['Hierarchical_Cluster'].head())"
      ],
      "metadata": {
        "id": "1aPOjoB3Tq7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Elbow Method , Silhouette Score to decide the number of clusters"
      ],
      "metadata": {
        "id": "MGY2w8BdUzKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score # Import silhouette_score\n",
        "\n",
        "sse = []\n",
        "silhouette_scores = []\n",
        "k_range = range(2, 11)  # Testing cluster counts from 2 to 10\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(rfm_scaled)\n",
        "\n",
        "    sse.append(kmeans.inertia_)  # Sum of squared distances (WCSS)\n",
        "\n",
        "    labels = kmeans.labels_\n",
        "    sil_score = silhouette_score(rfm_scaled, labels)\n",
        "    silhouette_scores.append(sil_score)\n",
        "\n",
        "# Plot Elbow Curve and Silhouette Scores side-by-side\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(k_range, sse, marker='o')\n",
        "plt.title('Elbow Method: WCSS vs Number of Clusters')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Within-cluster Sum of Squares (WCSS)')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(k_range, silhouette_scores, marker='s', color='orange')\n",
        "plt.title('Silhouette Scores vs Number of Clusters')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JQvkPthlUtd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Clustering\n"
      ],
      "metadata": {
        "id": "-i4Jqhi7WSTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### kmean clustering"
      ],
      "metadata": {
        "id": "6a7N_RpjXoo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "rfm['Cluster'] = kmeans.fit_predict(rfm_scaled)\n",
        "\n"
      ],
      "metadata": {
        "id": "I_2vIRomU4og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First 10 customers with cluster assignment:\")\n",
        "print(rfm[['Recency', 'Frequency', 'Monetary', 'Cluster']].head(10))\n",
        "\n",
        "# Print the number of customers in each cluster\n",
        "print(\"\\nCustomer count per cluster:\")\n",
        "print(rfm['Cluster'].value_counts())\n",
        "\n",
        "# Print average RFM values per cluster to help with labeling\n",
        "print(\"\\nCluster RFM Averages:\")\n",
        "print(rfm.groupby('Cluster')[['Recency', 'Frequency', 'Monetary']].mean().round(2))"
      ],
      "metadata": {
        "id": "uH5vrVz5V_l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBScan clustering"
      ],
      "metadata": {
        "id": "--FLGsQWXtui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(rfm.groupby('DBSCAN_Cluster')[['Recency', 'Frequency', 'Monetary']].mean())\n"
      ],
      "metadata": {
        "id": "_W196ApEXhwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hierarchial clustering"
      ],
      "metadata": {
        "id": "Ouv4arn2XxzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(rfm.groupby('Hierarchical_Cluster')[['Recency', 'Frequency', 'Monetary']].mean())\n"
      ],
      "metadata": {
        "id": "xaG9QPkwXiGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute relevant quantiles\n",
        "recency_q = rfm['Recency'].quantile([0.25, 0.75])\n",
        "frequency_q = rfm['Frequency'].quantile([0.25, 0.75])\n",
        "monetary_q = rfm['Monetary'].quantile([0.25, 0.75])\n",
        "\n",
        "def label_row(row):\n",
        "    r, f, m = row['Recency'], row['Frequency'], row['Monetary']\n",
        "    if r <= recency_q[0.25] and f >= frequency_q[0.75] and m >= monetary_q[0.75]:\n",
        "        return 'High-Value'\n",
        "    elif f >= frequency_q[0.25] and m >= monetary_q[0.25]:\n",
        "        return 'Regular'\n",
        "    elif f <= frequency_q[0.25] and m <= monetary_q[0.25] and r >= recency_q[0.75]:\n",
        "        return 'Occasional'\n",
        "    elif r >= recency_q[0.75] and f <= frequency_q[0.25] and m <= monetary_q[0.25]:\n",
        "        return 'At-Risk'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "rfm['Segment'] = rfm.apply(label_row, axis=1)\n"
      ],
      "metadata": {
        "id": "ZKHQR4Z7Vc1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing the clusters"
      ],
      "metadata": {
        "id": "ZlJ34sUSWmSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2D Scatter Plot: Recency vs. Monetary"
      ],
      "metadata": {
        "id": "UoRdBRngWx9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(data=rfm, x='Recency', y='Monetary', hue='Cluster', palette='Set1')\n",
        "plt.title(\"Customer Segments: Recency vs. Monetary by KMeans Cluster\")\n",
        "plt.xlabel(\"Recency (days)\")\n",
        "plt.ylabel(\"Monetary Value\")\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sZgn1abGWrJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3D Scatter Plot: Recency, Frequency, Monetary"
      ],
      "metadata": {
        "id": "ncTLHVIuW4q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "scatter = ax.scatter(\n",
        "    rfm['Recency'],\n",
        "    rfm['Frequency'],\n",
        "    rfm['Monetary'],\n",
        "    c=rfm['Cluster'],\n",
        "    cmap='Set1',\n",
        "    alpha=0.7\n",
        ")\n",
        "ax.set_xlabel('Recency')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.set_zlabel('Monetary')\n",
        "plt.title(\"Customer Segments (3D RFM Clusters) - KMeans\")\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "taUnIvkcW6DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recommendation System Approach:\n"
      ],
      "metadata": {
        "id": "Ihex2mnyYN3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('online_retail.csv', encoding='latin1')\n",
        "\n",
        "# Preprocessing:\n",
        "# Remove cancelled transactions (InvoiceNo starting with 'C')\n",
        "df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]\n",
        "\n",
        "# Aggregate purchases by CustomerID and StockCode (or Description)\n",
        "# Here we use Description for product name; StockCode can also be used.\n",
        "df = df.dropna(subset=['CustomerID', 'Description'])\n",
        "df['Quantity'] = df['Quantity'].clip(lower=0)  # Remove negative quantities if any\n",
        "\n",
        "# Create a customer-product matrix\n",
        "customer_product_matrix = df.pivot_table(\n",
        "    index='CustomerID',\n",
        "    columns='Description',\n",
        "    values='Quantity',\n",
        "    aggfunc='sum',\n",
        "    fill_value=0\n",
        ")\n",
        "\n",
        "# Compute cosine similarity between products (columns)\n",
        "product_similarity = cosine_similarity(customer_product_matrix.T)\n",
        "product_similarity_df = pd.DataFrame(\n",
        "    product_similarity,\n",
        "    index=customer_product_matrix.columns,\n",
        "    columns=customer_product_matrix.columns\n",
        ")\n",
        "\n",
        "def get_top_similar_products(product_name, top_n=5):\n",
        "    \"\"\"\n",
        "    Given a product description, return the top N most similar products.\n",
        "    \"\"\"\n",
        "    if product_name not in product_similarity_df.columns:\n",
        "        return f\"Product '{product_name}' not found in product list.\"\n",
        "\n",
        "    # Retrieve similarity scores for the input product\n",
        "    similarity_scores = product_similarity_df[product_name]\n",
        "\n",
        "    # Exclude the input product itself and sort by similarity\n",
        "    top_products = similarity_scores.drop(product_name).sort_values(ascending=False).head(top_n)\n",
        "\n",
        "    return top_products\n",
        "\n",
        "# Example usage:\n",
        "input_product = \"WHITE HANGING HEART T-LIGHT HOLDER\"  # Replace with any valid product description\n",
        "top_similar = get_top_similar_products(input_product, top_n=5)\n",
        "\n",
        "print(f\"Top 5 products similar to '{input_product}':\")\n",
        "print(top_similar)\n"
      ],
      "metadata": {
        "id": "Jwr0Sm0FYHI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('online_retail.csv', encoding='latin1')\n",
        "\n",
        "# Preprocessing: Remove cancelled transactions\n",
        "df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]\n",
        "df = df.dropna(subset=['CustomerID', 'Description'])\n",
        "df['Quantity'] = df['Quantity'].clip(lower=0)  # Remove negative purchases\n",
        "\n",
        "# Create the customer-product matrix\n",
        "customer_product_matrix = df.pivot_table(\n",
        "    index='CustomerID',\n",
        "    columns='Description',\n",
        "    values='Quantity',\n",
        "    aggfunc='sum',\n",
        "    fill_value=0\n",
        ")\n",
        "\n",
        "# Compute cosine similarity between products\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "product_similarity = cosine_similarity(customer_product_matrix.T)\n",
        "product_similarity_df = pd.DataFrame(\n",
        "    product_similarity,\n",
        "    index=customer_product_matrix.columns,\n",
        "    columns=customer_product_matrix.columns\n",
        ")\n",
        "\n",
        "# Save the similarity DataFrame to a pickle file for later use in your Streamlit app\n",
        "product_similarity_df.to_pickle('product_similarity_df.pkl')\n",
        "print(\"product_similarity_df.pkl created successfully.\")\n"
      ],
      "metadata": {
        "id": "2d1UBWM6sFTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('online_retail.csv', encoding='latin1')\n",
        "\n",
        "# Clean data: remove cancelled transactions (Invoice starting with 'C')\n",
        "df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]\n",
        "\n",
        "# Drop rows with missing CustomerID or Description\n",
        "df = df.dropna(subset=['CustomerID', 'Description'])\n",
        "\n",
        "# Optional: Remove negative quantities or returns if any\n",
        "df['Quantity'] = df['Quantity'].clip(lower=0)\n",
        "\n",
        "# Create the pivot table: customers x products\n",
        "customer_product_matrix = df.pivot_table(\n",
        "    index='CustomerID',\n",
        "    columns='Description',  # or use 'StockCode' if preferred\n",
        "    values='Quantity',\n",
        "    aggfunc='sum',\n",
        "    fill_value=0\n",
        ")\n",
        "\n",
        "# Save the DataFrame for later use\n",
        "customer_product_matrix.to_pickle('customer_product_matrix.pkl')\n",
        "print(\"Saved customer_product_matrix.pkl successfully.\")\n"
      ],
      "metadata": {
        "id": "p4QCU6agtNoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('online_retail.csv', encoding='latin1')\n",
        "\n",
        "# Remove cancelled transactions\n",
        "df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]\n",
        "\n",
        "# Drop rows with missing StockCode or Description\n",
        "df = df.dropna(subset=['StockCode', 'Description'])\n",
        "\n",
        "# Create mapping dictionary: StockCode -> Description\n",
        "stockcode_to_name = df.drop_duplicates(subset='StockCode').set_index('StockCode')['Description'].to_dict()\n",
        "\n",
        "# Save dict to a pickle file\n",
        "import pickle\n",
        "with open('stockcode_to_name.pkl', 'wb') as f:\n",
        "    pickle.dump(stockcode_to_name, f)\n",
        "\n",
        "print(\"stockcode_to_name.pkl created successfully.\")\n"
      ],
      "metadata": {
        "id": "kAkf0I8ftmY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install streamlit -q\n"
      ],
      "metadata": {
        "id": "qOyYhybYbdjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "QlbJcUQZqbGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "2_tKfQtNeDRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Shopper Spectrum project demonstrates the power of leveraging e-commerce transaction data to gain actionable insights into customer purchasing behaviors and improve business outcomes. By applying RFM (Recency, Frequency, Monetary) analysis, the project successfully segments customers into meaningful groups such as High-Value, Regular, Occasional, and At-Risk, allowing targeted marketing and personalized engagement strategies.\n",
        "\n",
        "The integration of an item-based collaborative filtering recommendation system, driven by product purchase histories and cosine similarity computations, enables the delivery of relevant product suggestions. This personalized recommendation enhances the user shopping experience and drives cross-selling opportunities.\n",
        "\n",
        "Through rigorous exploratory data analysis, clustering, and model evaluation techniques (including the Elbow method and Silhouette scores), the project identifies optimal customer segments and ensures robust predictive performance. The modular design facilitates deployment in interactive environments like Streamlit, empowering business stakeholders with easy-to-use tools for both customer segmentation and product recommendation.\n",
        "\n",
        "Overall, this dual approach—combining customer segmentation with collaborative filtering recommendations—enables e-commerce platforms to foster stronger customer loyalty, improve marketing efficiency, and enhance revenue growth in an increasingly competitive digital marketplace."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}